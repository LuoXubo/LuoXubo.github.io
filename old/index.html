<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/logo.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Xubo Luo</title>
  <meta name="Xubo Luo's Homepage" http-equiv="Content-Type" content="Xubo Luo's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Xubo Luo 「雒勖博」</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top" style="text-align: center;"><a href="images/profile.jpg"><img src="images/profile.jpg" width="40%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/luoxubo-cv.pdf">CV</a> |
    <a href="mailto:luoxubo23@mails.ucas.ac.cn">Email</a> |
    <a href="https://scholar.google.com.hk/citations?user=oGxPcIUAAAAJ&hl=zh-CN">Google Scholar</a> |
    <br/>
    | <a href="https://scholar.google.com.hk/citations?user=oGxPcIUAAAAJ&hl=zh-CN">Research Gate</a>
    | <a href="https://github.com/LuoXubo">Github</a> |  
    </p>
    <p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 156px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=xubo_luo&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1706734206165" data-screen-name=""></iframe><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </td>
    <td width="70%" valign="top" align="justify">
      <p>I am a second-year graduate student at University of Chinese Academy of Sciences, advised by Prof. <a href="https://people.ucas.ac.cn/~wanxue"> Wan</a>. </p>
      <p>I received my Bachelor's degree in computer science at Shanghai University of Finance and Economics, advised by Dr. <a href="https://itcs.sufe.edu.cn/LlChinese/46/d0/c10100a149200/page.htm"> Kwok</a>.
      </p>
      <p>My research area includes visual localization, robotics vision and vision-language navigation.</p>
      <p>Email: luoxuob23 [AT] mails.ucas.ac.cn
      </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;News</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
      <ul>
        <li>[09/2024] One paper is accpeted by CVIU!</li> 
        <li>[06/2024] One paper is accpeted by IROS 2024!</li> 
        <li>[06/2023] Graduate with honors from SUFE!</li>
        <li>[12/2022] One paper is accpeted by ICoSR 2022!</li> 
      </ul>
    </td>
  </tr>
</table>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top" align="center">
      <a href="/">
          <img src="images/dvd-mapal/pipeline.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="/" id="DVD_MapAL">
      <heading>DVD-MapAL: Deep Visual Feature-Driven Map-Assisted Localization for Planetary Spacecraft Exploration</heading></a><br>
      **Xubo Luo**, Zeyuan Zhao, Shengyang Zhang, Xue Wan, Wei Zhang, Leizheng Shu<br>
      IEEE Transactions on Field Robotics 2025<br>
      </p>
      <div class="paper" id="dvd_mapal">
      <a href="/">pdf</a> |
      <a href="javascript:toggleblock('dvd_mapal_abs')">abstract</a> |
      <a href="https://arxiv.org/">arXiv (Under review)</a> |

      <p align="justify"> <i id="dvd_mapal_abs">Spacecraft autonomous localization is a fundamental capability for space missions, including rover planetary surface exploration, satellite on-orbit servicing and lander precision landing tasks. While conventional approaches predominantly depend on ground-based observatories, their lack of autonomy and scalability limits their applications in deep-space exploration tasks. This paper presents Deep Visual Feature-Driven Map-Assisted Localization (DVD-MapAL), a novel method that combines visual odometry with prior map constraints to establish a self-contained localization system. The proposed architecture introduces two key innovations: (1) a coordinate-constrained pose optimization module that dynamically corrects cumulative errors in visual odometry through geometric consistency verification, and (2) a frequency-domain enhanced feature descriptor enabling robust projection estimation between orbital imagery and pre-existing maps. To encourage and inspire future research in this field, we release a comprehensive simulation dataset containing 11,721 photorealistic images with six-degree-of-freedom annotations, generated through Unreal Engine and AirSim platforms. Experimental evaluations demonstrate DVD-MapAL's superior performance, achieving 23.64\% lower mean positional error than ORB-SLAM3 in spacecraft scenarios. Further validation on the Chang'e lunar probe dataset reveals 17.89\% improvement over conventional visual localization methods, particularly in feature-deprived extraterrestrial environments. The open-source implementation and benchmark dataset will be made publicly available.
      </i></p>

      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/abs/2405.07429">
          <img src="images/jointloc/pipeline.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2405.07429" id="Jointloc">
      <heading>JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation.</heading></a><br>
      **Xubo Luo**, Xue Wan, Yixing Gao, Yaolin Tian, Wei Zhang, Leizheng Shu<br>
      IROS 2024<br>
      </p>

      <div class="paper" id="jointloc">
      <a href="https://arxiv.org/abs/2405.07429.pdf">pdf</a> |
      <a href="javascript:toggleblock('jointloc_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('jointloc')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2405.07429">arXiv</a> |
      <a href="https://github.com/LuoXubo/JointLoc">code</a> 

      <p align="justify"> <i id="jointloc_abs">Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras. However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced. In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose. Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter. JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively. The source code will be available at this https URL.      </i></p>

<pre xml:space="preserve">
  @misc{luo2024jointloc,
    title={JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation},
    author={Xubo Luo and Xue Wan and Yixing Gao and Yaolin Tian and Wei Zhang and Leizheng Shu},
    year={2024},
    eprint={2405.07429},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S107731422400242X">
          <img src="images/hbanet/pipeline.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2302.03122" id="HBANet">
      <heading>HBANet: A hybrid boundary-aware attention network for infrared and visible image fusion</heading></a><br>
      **Xubo Luo**, Liping Wang, Jinshuo Zhang, Dongmei Niu<br>
      Computer Vision and Image Understanding 2024<br>
      </p>

      <div class="paper" id="hbanet">
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S107731422400242X">pdf</a> |
      <a href="javascript:toggleblock('hbanet_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('hbanet')" class="togglebib">bibtex</a> |
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S107731422400242X">arXiv</a> |
      <a href="https://github.com/LuoXubo/HBANet">code</a> 

      <p align="justify"> <i id="hbanet_abs">Infrared and visible image fusion is an extensively investigated problem in infrared image processing, aiming to extract useful information from source images. However, the automatic fusion of these images presents a significant challenge due to the large domain difference and ambiguous boundaries. In this article, we propose a novel image fusion approach based on hybrid boundary-aware attention, termed HBANet, which models global dependencies across the image and leverages boundary-wise prior knowledge to supplement local details. Specifically, we design a novel mixed boundary-aware attention module that is capable of leveraging spatial information to the fullest extent and integrating long dependencies across different domains. To preserve the integrity of texture and structural information, we introduced a sophisticated loss function that comprises structure, intensity, and variation losses. Our method has been demonstrated to outperform state-of-the-art methods in terms of both visual and quantitative metrics, in our experiments on public datasets. Furthermore, our approach also exhibits great generalization capability, achieving satisfactory results in CT and MRI image fusion tasks.
      </i></p>

<pre xml:space="preserve">
  @article{LUO2024104161,
    title = {HBANet: A hybrid boundary-aware attention network for infrared and visible image fusion},
    journal = {Computer Vision and Image Understanding},
    volume = {249},
    pages = {104161},
    year = {2024},
    issn = {1077-3142},
    doi = {https://doi.org/10.1016/j.cviu.2024.104161},
    url = {https://www.sciencedirect.com/science/article/pii/S107731422400242X},
    author = {Xubo Luo and Jinshuo Zhang and Liping Wang and Dongmei Niu}
    }
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://ieeexplore.ieee.org/abstract/document/10137193">
          <img src="images/uav-geoloc/icosr.png" alt="sym" width="80%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://ieeexplore.ieee.org/abstract/document/10137193" id="Geoloc">
      <heading>Deep learning based cross-view image matching for UAV geo-localization</heading></a><br>
      **Xubo Luo**, Yaolin Tian, Xue Wan, Jinzhong Xu, Tao Ke<br>
      ICoSR 2022<br>
      </p>

      <div class="paper" id="geoloc">
      <a href="https://ieeexplore.ieee.org/abstract/document/10137193">pdf</a> |
      <a href="javascript:toggleblock('geoloc_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('geoloc')" class="togglebib">bibtex</a> |
      <a href="https://ieeexplore.ieee.org/abstract/document/10137193">arXiv</a> |
      <a href="https://github.com/LuoXubo/UAV-geoloc">code</a> 

      <p align="justify"> <i id="geoloc_abs">Unmanned Aerial Vehicles (UAVs) geo-localization refers to finding the position of a given aerial image in a large reference satellite image. Due to the large scale and illumination difference between aerial and satellite images, it is challenging that most existing cross-view image matching algorithms fail to localize the UAV images robustly and accurately. To solve the above problem, a novel UAV localization framework containing three-stage coarse-to-fine image matching is proposed. In the first stage, the satellite image is cropped into several local reference images to be matched with the aerial image. Then, ten candidate local images are selected from all of the local reference images with a simple and effective deep learning network, LPN. At last, a deep feature-based matching is employed between candidate local reference images and aerial images to determine the optimal position of the UAV in the reference map via homography transformation. In addition, a satellite-UAV image dataset is proposed, which contains 3 large-scale satellite images and 1909 aerial images. To demonstrate the performance of the proposed method, experiments on the large-scale proposed dataset are conducted. The experimental results illustrate that for more than 80% of the testing pair images, the proposed method is capable of refining the positioning error within 5 pixels, which meets the needs of UAV localization and is superior to other popular methods.
      </i></p>

<pre xml:space="preserve">
  @INPROCEEDINGS{10137193,
    author={Luo, Xubo and Tian, Yaolin and Wan, Xue and Xu, Jingzhong and Ke, Tao},
    booktitle={2022 International Conference on Service Robotics (ICoSR)},
    title={Deep learning based cross-view image matching for UAV geo-localization},
    year={2022},
    volume={},
    number={},
    pages={102-106},
    keywords={Location awareness;Deep learning;Satellites;Service robots;Image matching;Refining;Lighting;deep learning;image matching;geo-localization;autonomous drone navigation},
    doi={10.1109/ICoSR57188.2022.00028}}
  
</pre>
      </div>
    </td>
  </tr>


</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Service</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      IEEE Robotics and Automation Letters <b>(RA-L)</b> 2023
      <br>
      IEEE Transactions on Circuits and Systems for Video Technology <b>(TCSVT)</b> 2024
      <br>
      IEEE/SICE International Symposium on System Integrations  <b>(SII)</b> 2024
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br>
              <br>
              <div>
                  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=tt&d=Biz007_Pw8FVsAWycLRoKM_5XR_da9ccb8qGNbWVwnk&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
                  <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">
-->                               </div>
          </td>
      </tr>
  </tbody>
</table>








<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('wococo_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('omnih2o_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('hover_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('dvd-mapal');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('h2o_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('agile-but-safe_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('safedpa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('saferl_survey_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('patchail_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('jointloc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('hbanet');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('geoloc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('a2ls_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('issa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ebil_abs');
</script>
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mobile_aloha_abs');
</script>
</body>

</html>
